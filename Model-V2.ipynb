{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import os\n",
    "import logging\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "import psutil\n",
    "from tensorflow.python.profiler import profiler_v2 as profiler\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tqdm import tqdm\n",
    "import colorama\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')  # Updated method\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logging setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize colorama for colored output\n",
    "colorama.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"FIFTYONE_DEFAULT_DATASET_DIR\"] = \"/home/florian/\"\n",
    "fo.config.dataset_zoo_dir = \"/home/florian/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = augment_image(image)\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, batch_size=8, shuffle_buffer=1000):\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Food101**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from C:\\Users\\Troxi\\tensorflow_datasets\\food101\\2.0.0\n",
      "INFO:absl:Reusing dataset food101 (C:\\Users\\Troxi\\tensorflow_datasets\\food101\\2.0.0)\n",
      "INFO:absl:Creating a tf.data.Dataset reading 32 files located in folders: C:\\Users\\Troxi\\tensorflow_datasets\\food101\\2.0.0.\n",
      "INFO:absl:Constructing tf.data.Dataset food101 for split train, from C:\\Users\\Troxi\\tensorflow_datasets\\food101\\2.0.0\n",
      "INFO:absl:Load dataset info from C:\\Users\\Troxi\\tensorflow_datasets\\food101\\2.0.0\n",
      "INFO:absl:Reusing dataset food101 (C:\\Users\\Troxi\\tensorflow_datasets\\food101\\2.0.0)\n",
      "INFO:absl:Creating a tf.data.Dataset reading 16 files located in folders: C:\\Users\\Troxi\\tensorflow_datasets\\food101\\2.0.0.\n",
      "INFO:absl:Constructing tf.data.Dataset food101 for split validation, from C:\\Users\\Troxi\\tensorflow_datasets\\food101\\2.0.0\n"
     ]
    }
   ],
   "source": [
    "food101_train = tfds.load('food101', split='train', as_supervised=True)\n",
    "food101_val = tfds.load('food101', split='validation', as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Open Images V7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess_image(path, label):\n",
    "    try:\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, (224, 224))\n",
    "        img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "        return img, label\n",
    "    except tf.errors.InvalidArgumentError as e:\n",
    "        logging.error(f\"Error processing image at path {path}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def fiftyone_to_tf_dataset(fo_dataset):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    total_samples = 0\n",
    "    skipped_samples = 0\n",
    "\n",
    "    for sample in fo_dataset.iter_samples():\n",
    "        total_samples += 1\n",
    "        \n",
    "        # Check for labels in different possible fields\n",
    "        label = None\n",
    "        if hasattr(sample, 'ground_truth'):\n",
    "            label = sample.ground_truth.label\n",
    "        elif hasattr(sample, 'positive_labels') and sample.positive_labels:\n",
    "            if sample.positive_labels.classifications:\n",
    "                label = sample.positive_labels.classifications[0].label\n",
    "        elif hasattr(sample, 'detections') and sample.detections:\n",
    "            if sample.detections.detections:\n",
    "                label = sample.detections.detections[0].label\n",
    "        \n",
    "        if label in food_classes:\n",
    "            image_paths.append(sample.filepath)\n",
    "            labels.append(food_classes.index(label))\n",
    "        else:\n",
    "            skipped_samples += 1\n",
    "            continue\n",
    "\n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Skipped samples: {skipped_samples}\")\n",
    "    print(f\"Processed samples: {len(image_paths)}\")\n",
    "\n",
    "    if not image_paths:\n",
    "        raise ValueError(\"No samples matched the criteria. Check your food_classes and dataset labels.\")\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.filter(lambda x, y: x is not None and y is not None)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_classes = [\"Food\", \"Egg (Food)\", \"Fast food\", \"Seafood\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to 'z:open-images-v7\\train' if necessary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Downloading split 'train' to 'z:open-images-v7\\train' if necessary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/train/train-images-boxable-with-rotation.csv' to 'z:open-images-v7\\train\\metadata\\image_ids.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/2018_04/train/train-images-boxable-with-rotation.csv' to 'z:open-images-v7\\train\\metadata\\image_ids.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                             100% |██████|    4.8Gb/4.8Gb [16.7s elapsed, 0s remaining, 316.3Mb/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |██████|    4.8Gb/4.8Gb [16.7s elapsed, 0s remaining, 316.3Mb/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to 'z:open-images-v7\\train\\metadata\\classes.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to 'z:open-images-v7\\train\\metadata\\classes.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-attributes-description.csv' to 'z:open-images-v7\\train\\metadata\\attributes.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-attributes-description.csv' to 'z:open-images-v7\\train\\metadata\\attributes.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v5/classes-segmentation.txt' to 'z:open-images-v7\\train\\metadata\\segmentation_classes.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v5/classes-segmentation.txt' to 'z:open-images-v7\\train\\metadata\\segmentation_classes.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v7/oidv7-class-descriptions.csv' to 'z:open-images-v7\\train\\metadata\\point_classes.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v7/oidv7-class-descriptions.csv' to 'z:open-images-v7\\train\\metadata\\point_classes.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to 'C:\\Users\\Troxi\\AppData\\Local\\Temp\\tmp2rj7m38g\\metadata\\hierarchy.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to 'C:\\Users\\Troxi\\AppData\\Local\\Temp\\tmp2rj7m38g\\metadata\\hierarchy.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v5/train-annotations-human-imagelabels-boxable.csv' to 'z:open-images-v7\\train\\labels\\classifications.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v5/train-annotations-human-imagelabels-boxable.csv' to 'z:open-images-v7\\train\\labels\\classifications.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-train-annotations-bbox.csv' to 'z:open-images-v7\\train\\labels\\detections.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-train-annotations-bbox.csv' to 'z:open-images-v7\\train\\labels\\detections.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v7/oidv7-train-annotations-point-labels.csv' to 'z:open-images-v7\\train\\labels\\points.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v7/oidv7-train-annotations-point-labels.csv' to 'z:open-images-v7\\train\\labels\\points.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-train-annotations-vrd.csv' to 'z:open-images-v7\\train\\labels\\relationships.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-train-annotations-vrd.csv' to 'z:open-images-v7\\train\\labels\\relationships.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No segmentations exist for classes ['Egg (Food)', 'Fast food', 'Food', 'Seafood']\n",
      "You can view the available segmentation classes via `get_segmentation_classes()`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fiftyone.utils.openimages:No segmentations exist for classes ['Egg (Food)', 'Fast food', 'Food', 'Seafood']\n",
      "You can view the available segmentation classes via `get_segmentation_classes()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v5/train-annotations-object-segmentation.csv' to 'z:open-images-v7\\train\\labels\\segmentations.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v5/train-annotations-object-segmentation.csv' to 'z:open-images-v7\\train\\labels\\segmentations.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v5/train-masks/train-masks-0.zip' to 'z:open-images-v7\\train\\labels\\masks\\0.zip'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v5/train-masks/train-masks-0.zip' to 'z:open-images-v7\\train\\labels\\masks\\0.zip'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 100 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 100 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                     100% |███████████████████| 100/100 [5.8s elapsed, 0s remaining, 30.5 files/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |███████████████████| 100/100 [5.8s elapsed, 0s remaining, 30.5 files/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info written to 'z:open-images-v7\\info.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Dataset info written to 'z:open-images-v7\\info.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'open-images-v7' split 'train'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Loading 'open-images-v7' split 'train'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No segmentations exist for classes ['Egg (Food)', 'Fast food', 'Food', 'Seafood']\n",
      "You can view the available segmentation classes via `get_segmentation_classes()`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fiftyone.utils.openimages:No segmentations exist for classes ['Egg (Food)', 'Fast food', 'Food', 'Seafood']\n",
      "You can view the available segmentation classes via `get_segmentation_classes()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                         100% |█████████████████| 100/100 [615.3ms elapsed, 0s remaining, 162.5 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |█████████████████| 100/100 [615.3ms elapsed, 0s remaining, 162.5 samples/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'open-images-v7-train-100' created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Dataset 'open-images-v7-train-100' created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info: Name:        open-images-v7-train-100\n",
      "Media type:  image\n",
      "Num samples: 100\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:               fiftyone.core.fields.ObjectIdField\n",
      "    filepath:         fiftyone.core.fields.StringField\n",
      "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    created_at:       fiftyone.core.fields.DateTimeField\n",
      "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
      "    positive_labels:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n",
      "    negative_labels:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n",
      "    detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    points:           fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Keypoints)\n",
      "    relationships:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    segmentations:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Number of samples: 100\n",
      "Sample fields: ('id', 'filepath', 'tags', 'metadata', 'created_at', 'last_modified_at', 'positive_labels', 'negative_labels', 'detections', 'points', 'relationships', 'segmentations')\n",
      "Total samples: 100\n",
      "Skipped samples: 3\n",
      "Processed samples: 97\n"
     ]
    }
   ],
   "source": [
    "open_v7_train = foz.load_zoo_dataset(\n",
    "    \"open-images-v7\",\n",
    "    split=\"train\",\n",
    "    classes=food_classes,\n",
    "    max_samples=50000,  # Adjust as needed if we want more Samples\n",
    "    only_matching=True,\n",
    ")\n",
    "\n",
    "# Print some information about the dataset\n",
    "print(f\"Dataset info: {open_v7_train}\")\n",
    "print(f\"Number of samples: {len(open_v7_train)}\")\n",
    "print(f\"Sample fields: {open_v7_train.first().field_names}\")\n",
    "\n",
    "# Convert to TensorFlow dataset\n",
    "open_v7_train_tf = fiftyone_to_tf_dataset(open_v7_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to 'z:open-images-v7\\validation' if necessary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to 'z:open-images-v7\\validation' if necessary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/validation/validation-images-with-rotation.csv' to 'z:open-images-v7\\validation\\metadata\\image_ids.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/2018_04/validation/validation-images-with-rotation.csv' to 'z:open-images-v7\\validation\\metadata\\image_ids.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to 'z:open-images-v7\\validation\\metadata\\classes.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to 'z:open-images-v7\\validation\\metadata\\classes.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-attributes-description.csv' to 'z:open-images-v7\\validation\\metadata\\attributes.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-attributes-description.csv' to 'z:open-images-v7\\validation\\metadata\\attributes.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v5/classes-segmentation.txt' to 'z:open-images-v7\\validation\\metadata\\segmentation_classes.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v5/classes-segmentation.txt' to 'z:open-images-v7\\validation\\metadata\\segmentation_classes.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v7/oidv7-class-descriptions.csv' to 'z:open-images-v7\\validation\\metadata\\point_classes.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v7/oidv7-class-descriptions.csv' to 'z:open-images-v7\\validation\\metadata\\point_classes.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to 'C:\\Users\\Troxi\\AppData\\Local\\Temp\\tmpcvw69pw7\\metadata\\hierarchy.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to 'C:\\Users\\Troxi\\AppData\\Local\\Temp\\tmpcvw69pw7\\metadata\\hierarchy.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v5/validation-annotations-human-imagelabels-boxable.csv' to 'z:open-images-v7\\validation\\labels\\classifications.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v5/validation-annotations-human-imagelabels-boxable.csv' to 'z:open-images-v7\\validation\\labels\\classifications.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v5/validation-annotations-bbox.csv' to 'z:open-images-v7\\validation\\labels\\detections.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v5/validation-annotations-bbox.csv' to 'z:open-images-v7\\validation\\labels\\detections.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v7/oidv7-val-annotations-point-labels.csv' to 'z:open-images-v7\\validation\\labels\\points.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v7/oidv7-val-annotations-point-labels.csv' to 'z:open-images-v7\\validation\\labels\\points.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-validation-annotations-vrd.csv' to 'z:open-images-v7\\validation\\labels\\relationships.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-validation-annotations-vrd.csv' to 'z:open-images-v7\\validation\\labels\\relationships.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No segmentations exist for classes ['Egg (Food)', 'Fast food', 'Food', 'Seafood']\n",
      "You can view the available segmentation classes via `get_segmentation_classes()`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fiftyone.utils.openimages:No segmentations exist for classes ['Egg (Food)', 'Fast food', 'Food', 'Seafood']\n",
      "You can view the available segmentation classes via `get_segmentation_classes()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'https://storage.googleapis.com/openimages/v5/validation-annotations-object-segmentation.csv' to 'z:open-images-v7\\validation\\labels\\segmentations.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v5/validation-annotations-object-segmentation.csv' to 'z:open-images-v7\\validation\\labels\\segmentations.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 10 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 10 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                    100% |█████████████████████| 10/10 [2.2s elapsed, 0s remaining, 4.5 files/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |█████████████████████| 10/10 [2.2s elapsed, 0s remaining, 4.5 files/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info written to 'z:open-images-v7\\info.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Dataset info written to 'z:open-images-v7\\info.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'open-images-v7' split 'validation'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Loading 'open-images-v7' split 'validation'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No segmentations exist for classes ['Egg (Food)', 'Fast food', 'Food', 'Seafood']\n",
      "You can view the available segmentation classes via `get_segmentation_classes()`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fiftyone.utils.openimages:No segmentations exist for classes ['Egg (Food)', 'Fast food', 'Food', 'Seafood']\n",
      "You can view the available segmentation classes via `get_segmentation_classes()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                       100% |███████████████████| 10/10 [84.9ms elapsed, 0s remaining, 117.8 samples/s]     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |███████████████████| 10/10 [84.9ms elapsed, 0s remaining, 117.8 samples/s]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'open-images-v7-validation-10' created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Dataset 'open-images-v7-validation-10' created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info: Name:        open-images-v7-validation-10\n",
      "Media type:  image\n",
      "Num samples: 10\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:               fiftyone.core.fields.ObjectIdField\n",
      "    filepath:         fiftyone.core.fields.StringField\n",
      "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    created_at:       fiftyone.core.fields.DateTimeField\n",
      "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
      "    positive_labels:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n",
      "    negative_labels:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n",
      "    detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    points:           fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Keypoints)\n",
      "    segmentations:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Number of samples: 10\n",
      "Sample fields: ('id', 'filepath', 'tags', 'metadata', 'created_at', 'last_modified_at', 'positive_labels', 'negative_labels', 'detections', 'points', 'segmentations')\n",
      "Total samples: 10\n",
      "Skipped samples: 1\n",
      "Processed samples: 9\n"
     ]
    }
   ],
   "source": [
    "open_v7_val = foz.load_zoo_dataset(\n",
    "    \"open-images-v7\",\n",
    "    split=\"validation\",\n",
    "    classes=food_classes,\n",
    "    max_samples=5000,  # Adjust\n",
    "    only_matching=True,\n",
    ")\n",
    "\n",
    "# Print some information about the dataset\n",
    "print(f\"Dataset info: {open_v7_val}\")\n",
    "print(f\"Number of samples: {len(open_v7_val)}\")\n",
    "print(f\"Sample fields: {open_v7_val.first().field_names}\")\n",
    "\n",
    "# Convert to TensorFlow dataset\n",
    "open_v7_val_tf = fiftyone_to_tf_dataset(open_v7_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Class lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from C:\\Users\\Troxi\\tensorflow_datasets\\food101\\2.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes: 105\n"
     ]
    }
   ],
   "source": [
    "# Get Food101 class labels\n",
    "food101_info = tfds.builder('food101').info\n",
    "food101_labels = food101_info.features['label'].names\n",
    "\n",
    "# Combine with Open Images V7 food classes\n",
    "class_labels = food101_labels + food_classes\n",
    "\n",
    "# Make sure class_labels is a list and has unique values\n",
    "class_labels = list(set(class_labels))\n",
    "\n",
    "# Print the number of classes\n",
    "print(f\"Total number of classes: {len(class_labels)}\")\n",
    "\n",
    "# Update num_classes\n",
    "num_classes = len(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (8, 224, 224, 3) (8,)\n",
      "Validation dataset shape: (8, 224, 224, 3) (8,)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_food101(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "    return image, tf.cast(label, tf.int32)\n",
    "\n",
    "def preprocess_open_images(image, label):\n",
    "    image = tf.ensure_shape(image, (224, 224, 3))\n",
    "    return image, tf.cast(label, tf.int32)\n",
    "\n",
    "# Preprocess Food101 dataset\n",
    "food101_train = food101_train.map(preprocess_food101, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "food101_val = food101_val.map(preprocess_food101, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Preprocess Open Images dataset\n",
    "open_v7_train_tf = open_v7_train_tf.map(preprocess_open_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "open_v7_val_tf = open_v7_val_tf.map(preprocess_open_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Now combine the datasets\n",
    "train_dataset = food101_train.concatenate(open_v7_train_tf)\n",
    "val_dataset = food101_val.concatenate(open_v7_val_tf)\n",
    "\n",
    "# Then continue with your existing code for preparing the datasets\n",
    "train_dataset = prepare_dataset(train_dataset)\n",
    "val_dataset = prepare_dataset(val_dataset)\n",
    "\n",
    "# Print shapes to verify\n",
    "for images, labels in train_dataset.take(1):\n",
    "    print(\"Train dataset shape:\", images.shape, labels.shape)\n",
    "\n",
    "for images, labels in val_dataset.take(1):\n",
    "    print(\"Validation dataset shape:\", images.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_labels)\n",
    "\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Use mixed precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.keras', \n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    checkpoint,\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=3)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def grad_accumulation_step(model, x, y, optimizer, acc_gradients, num_accumulation_steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y, logits)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    for i in range(len(acc_gradients)):\n",
    "        acc_gradients[i].assign_add(gradients[i])  # Use assign_add instead of +=\n",
    "    \n",
    "    if tf.equal(optimizer.iterations % num_accumulation_steps, 0):\n",
    "        optimizer.apply_gradients(zip(acc_gradients, model.trainable_variables))\n",
    "        for i in range(len(acc_gradients)):\n",
    "            acc_gradients[i].assign(tf.zeros_like(acc_gradients[i]))  # Reset accumulated gradients\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"Clears memory by invoking garbage collector and optionally resetting GPU memory.\"\"\"\n",
    "    gc.collect() \n",
    "    tf.keras.backend.clear_session()  #\n",
    "\n",
    "# Custom logger\n",
    "class ColoredLogger(logging.Logger):\n",
    "    def __init__(self, name, level=logging.NOTSET):\n",
    "        super().__init__(name, level)\n",
    "        self.formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        self.console_handler = logging.StreamHandler()\n",
    "        self.console_handler.setFormatter(self.formatter)\n",
    "        self.addHandler(self.console_handler)\n",
    "\n",
    "    def info(self, msg, *args, **kwargs):\n",
    "        self._log(logging.INFO, f\"\\033[92m{msg}\\033[0m\", args, **kwargs)\n",
    "\n",
    "    def warning(self, msg, *args, **kwargs):\n",
    "        self._log(logging.WARNING, f\"\\033[93m{msg}\\033[0m\", args, **kwargs)\n",
    "\n",
    "    def error(self, msg, *args, **kwargs):\n",
    "        self._log(logging.ERROR, f\"\\033[91m{msg}\\033[0m\", args, **kwargs)\n",
    "\n",
    "# Set up logging\n",
    "logging.setLoggerClass(ColoredLogger)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# ... (rest of the imports and GPU setup)\n",
    "\n",
    "# Memory usage logging\n",
    "def log_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    memory_info = process.memory_info()\n",
    "    memory_usage = memory_info.rss / 1024 / 1024  # in MB\n",
    "    logger.info(f\"Memory usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "# Training with gradient accumulation and improved logging\n",
    "def train_model(model, train_dataset, val_dataset, epochs, steps_per_epoch, num_accumulation_steps):\n",
    "    acc_gradients = [tf.Variable(tf.zeros_like(tv)) for tv in model.trainable_variables]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        logger.info(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Training metrics\n",
    "        train_loss = tf.keras.metrics.Mean()\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        \n",
    "        train_iter = iter(train_dataset.repeat())\n",
    "        \n",
    "        with tqdm(total=steps_per_epoch, desc=f\"Training\", unit=\"step\") as pbar:\n",
    "            for step in range(steps_per_epoch):\n",
    "                x, y = next(train_iter)\n",
    "                loss = grad_accumulation_step(model, x, y, optimizer, acc_gradients, num_accumulation_steps)\n",
    "                train_loss(loss)\n",
    "                train_accuracy(y, model(x, training=True))\n",
    "                \n",
    "                if (step + 1) % num_accumulation_steps == 0:\n",
    "                    pbar.update(num_accumulation_steps)\n",
    "                    pbar.set_postfix({\"Loss\": f\"{train_loss.result():.4f}\", \"Accuracy\": f\"{train_accuracy.result():.4f}\"})\n",
    "                \n",
    "        \n",
    "        # Validation\n",
    "        val_loss = tf.keras.metrics.Mean()\n",
    "        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        \n",
    "        for x, y in val_dataset:\n",
    "            y_pred = model(x, training=False)\n",
    "            val_loss(tf.keras.losses.sparse_categorical_crossentropy(y, y_pred))\n",
    "            val_accuracy(y, y_pred)\n",
    "                \n",
    "        logger.info(f\"Epoch {epoch+1} results:\")\n",
    "        logger.info(f\"  Train Loss: {train_loss.result():.4f}, Train Accuracy: {train_accuracy.result():.4f}\")\n",
    "        logger.info(f\"  Val Loss: {val_loss.result():.4f}, Val Accuracy: {val_accuracy.result():.4f}\")\n",
    "        log_memory_usage()\n",
    "        clear_memory()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 1/10\n",
      "Training:   1%|          | 48/9481 [00:54<2:57:31,  1.13s/step, Loss=5.0199, Accuracy=0.0312]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(food101_train) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(open_v7_train) \n\u001b[0;32m      7\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m total_samples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfood_recognition_modelV2.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 49\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataset, val_dataset, epochs, steps_per_epoch, num_accumulation_steps)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps_per_epoch):\n\u001b[0;32m     48\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_iter)\n\u001b[1;32m---> 49\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_accumulation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macc_gradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_accumulation_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     train_loss(loss)\n\u001b[0;32m     51\u001b[0m     train_accuracy(y, model(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "base_model.trainable = True\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 8  # Define batch_size (adjust as needed)\n",
    "total_samples = len(food101_train) + len(open_v7_train) \n",
    "steps_per_epoch = total_samples // batch_size\n",
    "\n",
    "train_model(model, train_dataset, val_dataset, epochs=10, steps_per_epoch=steps_per_epoch, num_accumulation_steps=4)\n",
    "\n",
    "# Save the model\n",
    "model.save('food_recognition_modelV2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_food(image_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, axis=0)\n",
    "    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = class_labels[np.argmax(predictions)]\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nutrition data integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_data = pd.read_csv(\"Nutrition-Data/nutrients_csvfile.csv\")\n",
    "\n",
    "def get_nutrition_info(food_item):\n",
    "    try:\n",
    "        nutrition = nutrition_data[nutrition_data['food_item'] == food_item].iloc[0]\n",
    "        return {\n",
    "            'calories': nutrition['calories'],\n",
    "            'protein': nutrition['protein'],\n",
    "            'carbs': nutrition['carbs'],\n",
    "            'fat': nutrition['fat']\n",
    "        }\n",
    "    except IndexError:\n",
    "        return get_nutrition_info_from_api(food_item)\n",
    "\n",
    "# API CALL WHEN NUTRITION VALUERS ARE NOT IN THE LOCAL DATASET\n",
    "API_KEY = \"hydUyBjWVdUlt1qNIeB2dKGgQYbjFiQwMjm6YpBn\" \n",
    "API_ENDPOINT = \"https://api.nal.usda.gov/fdc/v1/foods/search\"\n",
    "\n",
    "def get_nutrition_info_from_api(food_item):\n",
    "    params = {\n",
    "        \"api_key\": API_KEY,\n",
    "        \"query\": food_item,\n",
    "        \"dataType\": [\"Survey (FNDDS)\"],\n",
    "        \"pageSize\": 1\n",
    "    }\n",
    "    \n",
    "    response = requests.get(API_ENDPOINT, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data['foods']:\n",
    "            food = data['foods'][0]\n",
    "            nutrients = food['foodNutrients']\n",
    "            \n",
    "            nutrition_info = {\n",
    "                'calories': next((n['value'] for n in nutrients if n['nutrientName'] == 'Energy'), None),\n",
    "                'protein': next((n['value'] for n in nutrients if n['nutrientName'] == 'Protein'), None),\n",
    "                'carbs': next((n['value'] for n in nutrients if n['nutrientName'] == 'Carbohydrate, by difference'), None),\n",
    "                'fat': next((n['value'] for n in nutrients if n['nutrientName'] == 'Total lipid (fat)'), None)\n",
    "            }\n",
    "            \n",
    "            return nutrition_info\n",
    "    \n",
    "    # If API call fails or no data found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def food_recognition_and_nutrition(image_path):\n",
    "    try:\n",
    "        predicted_class_index = predict_food(image_path)\n",
    "        food_item = class_labels[predicted_class_index]\n",
    "        nutrition_info = get_nutrition_info(food_item)\n",
    "        \n",
    "        return {\n",
    "            'food_item': food_item,\n",
    "            'nutrition_info': nutrition_info\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in food recognition: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFLITE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFLite conversion\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('food_recognition_model_v2.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"TensorFlow Lite model saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
